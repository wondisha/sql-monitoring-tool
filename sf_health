import streamlit as st
import pandas as pd
from datetime import datetime, timedelta

# --- Configuration ---
# When running Streamlit in Snowflake, the connection is handled automatically.
# No need for explicit connection parameters or secrets.toml for the primary Snowflake connection.
# You can still use st.secrets for other API keys or custom configurations if needed.

# --- Snowflake Connection (Managed by Streamlit in Snowflake) ---
# Use st.connection("snowflake") to get a connection to the current Snowflake session.
# This connection is implicitly managed by Streamlit in Snowflake and does not require
# user/password inputs in the app itself.

# --- Data Fetching Functions ---

@st.cache_data(ttl=600) # Cache data for 10 minutes
def get_query_history(time_range_hours=24):
    """
    Fetches query history and insights for a given time range using the Streamlit in Snowflake connection.
    This query is based on the user's provided optimized query.
    """
    end_time = datetime.now()
    start_time_7_days_ago = end_time - timedelta(days=7)
    # The user's query uses DATEADD(day, -7, CURRENT_TIMESTAMP()) for its primary filter.
    # We will align the Streamlit slider's 'query_time_range' with this for consistency,
    # but the core SQL will use the 7-day window as requested.

    # NOTE: The 'ABC' role name in the subquery is a placeholder.
    # You might need to change 'ABC' to a relevant role in your Snowflake environment.
    query = f"""
    SELECT
        qi.query_id,
        qh.query_text,
        qh.user_name,
        qh.warehouse_name,
        qh.start_time AS query_execution_time, -- Added: When the query was executed
        qh.total_elapsed_time / 1000 AS execution_time_seconds,
        qh.bytes_scanned,
        qh.partitions_scanned,
        qh.partitions_total,
        qi.insight_type_id,
        qi.message,
        qi.suggestions,
        (SELECT COUNT(*)
         FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY AS sub_qh
         WHERE sub_qh.query_text = qh.query_text
           AND sub_qh.start_time >= '{start_time_7_days_ago.isoformat()}'
           AND sub_qh.start_time <= '{end_time.isoformat()}'
        ) AS query_execution_count -- Added: How many times this specific query text was executed
    FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_INSIGHTS qi
    JOIN SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY qh
        ON qi.query_id = qh.query_id
    WHERE qi.start_time >= '{start_time_7_days_ago.isoformat()}'
      AND qh.user_name IN (
          SELECT DISTINCT user_name
          FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY
          WHERE role_name = 'ABC' -- <<< IMPORTANT: Adjust 'ABC' to your relevant role name
      )
      AND (qh.total_elapsed_time > 3600000 -- Queries longer than 1 hour (3600 seconds)
           OR qi.insight_type_id IN (
               'QUERY_INSIGHT_LOCAL_DISK_SPILLAGE',
               'QUERY_INSIGHT_REMOTE_DISK_SPILLAGE',
               'QUERY_INSIGHT_INAPPLICABLE_FILTER_ON_TABLE_SCAN',
               'QUERY_INSIGHT_UNSELECTIVE_FILTER'
           )
           OR qh.bytes_scanned > 1000000000 -- Queries scanning > 1GB
           OR qh.partitions_scanned / NULLIF(qh.partitions_total, 0) > 0.8) -- Poor pruning (>80% partitions scanned)
    ORDER BY qh.total_elapsed_time DESC
    LIMIT 50;
    """
    try:
        # Get a cursor from the Streamlit in Snowflake connection
        cur = st.connection("snowflake").cursor()
        cur.execute(query)
        df = cur.fetch_pandas_all()

        # Convert relevant columns to numeric, coercing errors to NaN, then fill NaN with 0
        numeric_cols = [
            'EXECUTION_TIME_SECONDS', # Renamed from TOTAL_ELAPSED_TIME_SEC in new query
            'BYTES_SCANNED',
            'PARTITIONS_SCANNED',
            'PARTITIONS_TOTAL',
            'QUERY_EXECUTION_COUNT'
        ]
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').astype(float).fillna(0)

        # Convert query_execution_time to datetime if it's not already
        if 'QUERY_EXECUTION_TIME' in df.columns:
            df['QUERY_EXECUTION_TIME'] = pd.to_datetime(df['QUERY_EXECUTION_TIME'], errors='coerce')


        return df
    except Exception as e:
        st.error(f"Error fetching query history: {e}")
        return pd.DataFrame()

@st.cache_data(ttl=600)
def get_warehouse_metering_history(time_range_days=7):
    """Fetches warehouse metering history for a given time range using the Streamlit in Snowflake connection."""
    end_time = datetime.now()
    start_time = end_time - timedelta(days=time_range_days)
    query = f"""
    SELECT
        START_TIME,
        END_TIME,
        WAREHOUSE_ID,
        WAREHOUSE_NAME,
        CREDITS_USED,
        CREDITS_USED_COMPUTE,
        CREDITS_USED_CLOUD_SERVICES
    FROM
        SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY
    WHERE
        START_TIME >= '{start_time.isoformat()}' AND END_TIME <= '{end_time.isoformat()}'
    ORDER BY
        START_TIME ASC;
    """
    try:
        # Get a cursor from the Streamlit in Snowflake connection
        cur = st.connection("snowflake").cursor()
        cur.execute(query)
        df = cur.fetch_pandas_all()

        # Convert relevant columns to numeric, coercing errors to NaN, then fill NaN with 0
        numeric_cols = ['CREDITS_USED', 'CREDITS_USED_COMPUTE', 'CREDITS_USED_CLOUD_SERVICES']
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').astype(float).fillna(0)

        return df
    except Exception as e:
        st.error(f"Error fetching warehouse metering history: {e}")
        return pd.DataFrame()

@st.cache_data(ttl=600)
def get_login_history(time_range_days=7):
    """Fetches login history for a given time range using the Streamlit in Snowflake connection."""
    end_time = datetime.now()
    start_time = end_time - timedelta(days=time_range_days)
    query = f"""
    SELECT
        EVENT_TIMESTAMP,
        USER_NAME,
        CLIENT_IP,
        REPORTED_CLIENT_TYPE,
        REPORTED_CLIENT_VERSION,
        FIRST_AUTHENTICATION_FACTOR,
        SECOND_AUTHENTICATION_FACTOR,
        IS_SUCCESS
    FROM
        SNOWFLAKE.ACCOUNT_USAGE.LOGIN_HISTORY
    WHERE
        EVENT_TIMESTAMP >= '{start_time.isoformat()}' AND EVENT_TIMESTAMP <= '{end_time.isoformat()}'
    ORDER BY
        EVENT_TIMESTAMP DESC;
    """
    try:
        # Get a cursor from the Streamlit in Snowflake connection
        cur = st.connection("snowflake").cursor()
        cur.execute(query)
        df = cur.fetch_pandas_all()
        return df
    except Exception as e:
        st.error(f"Error fetching login history: {e}")
        return pd.DataFrame()


# --- Streamlit App Layout ---
st.set_page_config(layout="wide", page_title="Snowflake Health Check App")

st.title("‚ùÑÔ∏è Snowflake Health Check Dashboard")

# Sidebar Filters
st.sidebar.header("Filters")
# Note: The query history SQL now uses a fixed 7-day window for its primary filter,
# but this slider can still be used for additional client-side filtering if desired,
# or for other sections that might use a dynamic time range.
query_time_range = st.sidebar.slider(
    "Query History Time Range (Hours)",
    min_value=1,
    max_value=168, # 7 days
    value=24, # Default to 24 hours for display purposes, but SQL is 7 days
    help="Note: The core SQL query for Query Insights fetches data for the last 7 days. This slider can be used for additional client-side filtering if needed."
)
warehouse_time_range = st.sidebar.slider(
    "Warehouse Usage Time Range (Days)",
    min_value=1,
    max_value=30,
    value=7,
    help="Select the time window for fetching warehouse metering history."
)

# Fetch data based on filters
query_df = get_query_history(query_time_range) # This now fetches the 7-day insights data
warehouse_df = get_warehouse_metering_history(warehouse_time_range)
login_df = get_login_history(warehouse_time_range) # Using warehouse time range for login history too

# --- Tabs for Navigation ---
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "Overview",
    "Long Running Queries",
    "Resource Utilization",
    "Potentially Impacting Queries & Users",
    "Query Insights"
])

with tab1:
    st.header("Dashboard Overview")
    if not query_df.empty:
        total_queries = len(query_df)
        successful_queries = query_df[query_df['EXECUTION_STATUS'] == 'SUCCESS'].shape[0]
        failed_queries = query_df[query_df['EXECUTION_STATUS'] == 'FAILED'].shape[0]
        avg_exec_time = query_df['EXECUTION_TIME_SECONDS'].mean() if not query_df.empty else 0
        total_credits_used = warehouse_df['CREDITS_USED'].sum() if not warehouse_df.empty else 0

        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Total Queries (with Insights)", total_queries)
        with col2:
            st.metric("Successful Queries", successful_queries)
        with col3:
            st.metric("Failed Queries", failed_queries)
        with col4:
            st.metric("Avg. Execution Time (s)", f"{avg_exec_time:.2f}")

        st.metric("Total Credits Used (Last 7 Days)", f"{total_credits_used:.2f}")

        st.subheader("Health Summary")
        if failed_queries > 0 and total_queries > 0:
            failed_percentage = (failed_queries / total_queries) * 100
            if failed_percentage > 5:
                st.warning(f"üö® **High Failed Query Rate:** {failed_percentage:.2f}% of queries failed. Investigate common error patterns.")
            else:
                st.info(f"‚úÖ Failed query rate is low ({failed_percentage:.2f}%).")
        else:
            st.info("No failed queries detected in the selected period.")

        if not query_df.empty and query_df['EXECUTION_TIME_SECONDS'].max() > 300: # 5 minutes
            st.warning("‚è±Ô∏è **Very Long Running Queries Detected:** Some queries are running for over 5 minutes. Check the 'Long Running Queries' tab for details.")
        else:
            st.info("‚úÖ No excessively long-running queries detected.")

        if not warehouse_df.empty and warehouse_df['CREDITS_USED'].sum() > 1000: # Example threshold
            st.warning(f"üí∞ **Significant Credit Usage:** Total credits used ({total_credits_used:.2f}) are high. Review warehouse utilization.")
        else:
            st.info(f"‚úÖ Credit usage ({total_credits_used:.2f}) appears normal.")

        st.subheader("Top 5 Longest Running Queries (Overall)")
        st.dataframe(query_df.nlargest(5, 'EXECUTION_TIME_SECONDS')[
            ['QUERY_ID', 'QUERY_TEXT', 'USER_NAME', 'WAREHOUSE_NAME', 'EXECUTION_TIME_SECONDS', 'EXECUTION_STATUS', 'INSIGHT_TYPE_ID']
        ], use_container_width=True)

    if not warehouse_df.empty:
        st.subheader("Warehouse Credit Usage (Last 7 Days)")
        warehouse_credits = warehouse_df.groupby('WAREHOUSE_NAME')['CREDITS_USED'].sum().reset_index()
        st.dataframe(warehouse_credits, use_container_width=True)
        st.line_chart(warehouse_df.set_index('START_TIME')['CREDITS_USED'], use_container_width=True)

    if not login_df.empty:
        st.subheader("Recent Login Attempts (Last 7 Days)")
        st.dataframe(login_df.head(10), use_container_width=True) # Show top 10 recent logins

with tab2:
    st.header("Long Running Queries")
    if not query_df.empty:
        st.write(f"Displaying queries from the last 7 days (as per the underlying SQL query).")
        min_exec_time = st.slider(
            "Minimum Execution Time (seconds)",
            min_value=0,
            max_value=int(query_df['EXECUTION_TIME_SECONDS'].max()) + 1 if not query_df.empty else 60,
            value=10, # Default to queries longer than 10 seconds
            help="Filter queries by their total execution time."
        )
        long_running_queries = query_df[query_df['EXECUTION_TIME_SECONDS'] >= min_exec_time].sort_values(
            'EXECUTION_TIME_SECONDS', ascending=False
        )

        if not long_running_queries.empty:
            st.dataframe(long_running_queries[[
                'QUERY_ID', 'QUERY_TEXT', 'USER_NAME', 'WAREHOUSE_NAME',
                'EXECUTION_TIME_SECONDS', 'BYTES_SCANNED', 'PARTITIONS_SCANNED', 'PARTITIONS_TOTAL',
                'QUERY_EXECUTION_TIME', 'EXECUTION_STATUS', 'INSIGHT_TYPE_ID', 'MESSAGE'
            ]], use_container_width=True)
        else:
            st.info("No long running queries found based on current filters.")
    else:
        st.info("No query history data available.")

with tab3:
    st.header("Resource Utilization")
    if not warehouse_df.empty:
        st.subheader("Warehouse Credit Usage Over Time")
        st.write(f"Displaying data from the last {warehouse_time_range} days.")
        warehouse_name_filter = st.selectbox(
            "Select Warehouse",
            ['All'] + sorted(warehouse_df['WAREHOUSE_NAME'].unique().tolist())
        )

        filtered_warehouse_df = warehouse_df
        if warehouse_name_filter != 'All':
            filtered_warehouse_df = warehouse_df[warehouse_df['WAREHOUSE_NAME'] == warehouse_name_filter]

        if not filtered_warehouse_df.empty:
            st.line_chart(filtered_warehouse_df.set_index('START_TIME')[['CREDITS_USED_COMPUTE', 'CREDITS_USED_CLOUD_SERVICES']], use_container_width=True)

            st.subheader("Top Warehouses by Credit Usage")
            total_credits_by_warehouse = filtered_warehouse_df.groupby('WAREHOUSE_NAME')['CREDITS_USED'].sum().sort_values(ascending=False)
            st.dataframe(total_credits_by_warehouse.reset_index(), use_container_width=True)
        else:
            st.info("No warehouse usage data available for the selected filters.")
    else:
        st.info("No warehouse metering history data available.")

    st.subheader("Query Resource Consumption (Top 10 by Bytes Scanned)")
    if not query_df.empty:
        # The fix for BYTES_SCANNED is applied in get_query_history
        top_scanned_queries = query_df.nlargest(10, 'BYTES_SCANNED')
        if not top_scanned_queries.empty:
            st.dataframe(top_scanned_queries[[
                'QUERY_ID', 'QUERY_TEXT', 'USER_NAME', 'WAREHOUSE_NAME', 'BYTES_SCANNED', 'EXECUTION_TIME_SECONDS', 'INSIGHT_TYPE_ID'
            ]], use_container_width=True)
        else:
            st.info("No queries with significant bytes scanned found.")
    else:
        st.info("No query history data available.")

with tab4:
    st.header("Potentially Impacting Queries & Users")
    st.write("""
        Snowflake's architecture minimizes traditional "blocking" as seen in row-level locking databases.
        However, long-running or resource-intensive queries can indirectly impact performance and
        resource availability for other users. This section identifies such queries and the users running them.
    """)

    if not query_df.empty:
        st.subheader("Queries with High Resource Consumption (Top 20 by Total Elapsed Time)")
        impacting_queries = query_df.nlargest(20, 'EXECUTION_TIME_SECONDS')
        if not impacting_queries.empty:
            st.dataframe(impacting_queries[[
                'QUERY_ID', 'QUERY_TEXT', 'USER_NAME', 'WAREHOUSE_NAME',
                'EXECUTION_TIME_SECONDS', 'BYTES_SCANNED', 'ROWS_PRODUCED', 'QUERY_EXECUTION_TIME', 'EXECUTION_STATUS', 'INSIGHT_TYPE_ID'
            ]], use_container_width=True)

        st.subheader("Top Users by Resource Consumption")
        if not query_df.empty:
            user_total_time = query_df.groupby('USER_NAME')['EXECUTION_TIME_SECONDS'].sum().sort_values(ascending=False).reset_index()
            st.write("#### Top Users by Total Query Execution Time")
            st.dataframe(user_total_time, use_container_width=True)

            user_total_bytes_scanned = query_df.groupby('USER_NAME')['BYTES_SCANNED'].sum().sort_values(ascending=False).reset_index()
            st.write("#### Top Users by Total Bytes Scanned")
            st.dataframe(user_total_bytes_scanned, use_container_width=True)

            user_failed_queries = query_df[query_df['EXECUTION_STATUS'] == 'FAILED'].groupby('USER_NAME').size().sort_values(ascending=False).reset_index(name='FAILED_QUERY_COUNT')
            st.write("#### Top Users by Failed Query Count")
            st.dataframe(user_failed_queries, use_container_width=True)
        else:
            st.info("No query history data available to analyze user impact.")

    else:
        st.info("No query history data available to analyze.")

with tab5:
    st.header("Query Insights & Enhancement Suggestions")
    st.write("""
        This section provides insights from Snowflake's native `QUERY_INSIGHTS` view and
        demonstrates how an LLM could provide further suggestions.
    """)

    if not query_df.empty:
        st.subheader("Queries with Snowflake Native Insights")
        # Filter for queries that have an actual insight_type_id
        queries_with_insights = query_df[query_df['INSIGHT_TYPE_ID'].notna()]
        if not queries_with_insights.empty:
            st.dataframe(queries_with_insights[[
                'QUERY_ID', 'QUERY_TEXT', 'USER_NAME', 'WAREHOUSE_NAME',
                'EXECUTION_TIME_SECONDS', 'BYTES_SCANNED', 'PARTITIONS_SCANNED', 'PARTITIONS_TOTAL',
                'INSIGHT_TYPE_ID', 'MESSAGE', 'SUGGESTIONS', 'QUERY_EXECUTION_COUNT'
            ]], use_container_width=True)
            st.markdown("""
                **Understanding Snowflake Native Insights:**
                * `INSIGHT_TYPE_ID`: Identifies the type of insight (e.g., spillage, unselective filter).
                * `MESSAGE`: A brief description of the insight.
                * `SUGGESTIONS`: Snowflake's direct suggestions for optimizing the query.
            """)
        else:
            st.info("No Snowflake native query insights found for the selected criteria.")


        st.subheader("Queries with High Bytes Scanned or Low Rows Produced (Potential for Optimization)")
        # Identify queries that scan a lot of data but produce few rows (e.g., inefficient filtering)
        # Exclude those already covered by native insights if desired, or keep for broader view
        potential_inefficient_queries = query_df[
            (query_df['BYTES_SCANNED'] > 0) &
            (query_df['ROWS_PRODUCED'] < 100) &
            (query_df['EXECUTION_TIME_SECONDS'] > 5)
        ].sort_values('BYTES_SCANNED', ascending=False)

        if not potential_inefficient_queries.empty:
            st.dataframe(potential_inefficient_queries[[
                'QUERY_ID', 'QUERY_TEXT', 'USER_NAME', 'WAREHOUSE_NAME', 'BYTES_SCANNED', 'ROWS_PRODUCED', 'EXECUTION_TIME_SECONDS', 'INSIGHT_TYPE_ID'
            ]], use_container_width=True)
            st.markdown("""
                **Suggestions for these queries:**
                * **Add/Refine WHERE clauses:** Ensure precise filtering to reduce data scanned.
                * **Clustering Keys:** Consider clustering tables on columns frequently used in WHERE clauses.
                * **Materialized Views:** For frequently run queries on large datasets, a materialized view might help.
                * **Proper Joins:** Ensure joins are efficient and use appropriate join types.
            """)
        else:
            st.info("No obvious inefficient queries found based on simple heuristics (high bytes scanned, low rows produced, longer execution).")

        st.subheader("Queries with High Compilation Time (Potential for Complex Plans or Cold Cache)")
        high_compilation_queries = query_df[query_df['COMPILATION_TIME_SEC'] > 1].sort_values('COMPILATION_TIME_SEC', ascending=False).head(10)
        if not high_compilation_queries.empty:
            st.dataframe(high_compilation_queries[[
                'QUERY_ID', 'QUERY_TEXT', 'USER_NAME', 'WAREHOUSE_NAME', 'COMPILATION_TIME_SEC', 'EXECUTION_TIME_SECONDS', 'INSIGHT_TYPE_ID'
            ]], use_container_width=True)
            st.markdown("""
                **Suggestions for these queries:**
                * **Parameterize Queries:** Reduce compilation time by using bind variables for repeated queries.
                * **Warehouse Warm-up:** Ensure the warehouse is warm if compilation time is consistently high on first runs.
                * **Simplify Complex Queries:** Break down very complex queries into simpler CTEs or views.
            """)
        else:
            st.info("No queries with unusually high compilation times found.")

        st.subheader("Failed Queries Analysis")
        failed_queries_df = query_df[query_df['EXECUTION_STATUS'] == 'FAILED']
        if not failed_queries_df.empty:
            st.dataframe(failed_queries_df[[
                'QUERY_ID', 'QUERY_TEXT', 'USER_NAME', 'WAREHOUSE_NAME', 'QUERY_EXECUTION_TIME', 'EXECUTION_STATUS', 'INSIGHT_TYPE_ID', 'MESSAGE'
            ]], use_container_width=True)
            st.markdown("""
                **Suggestions for failed queries:**
                * **Check Error Messages:** Investigate the specific error messages in Snowflake's Query History for root cause.
                * **User Permissions:** Verify the user has necessary permissions on objects.
                * **Syntax Errors:** Review the query text for syntax issues.
                * **Data Type Mismatches:** Ensure data types are compatible in operations.
            """)
        else:
            st.info("No failed queries found in the selected time range.")

        st.subheader("AI-Powered Query Insights (Conceptual)")
        st.markdown("""
            This section demonstrates how you could integrate a Large Language Model (LLM) for
            more specific and actionable query enhancement suggestions.
        """)

        if not query_df.empty:
            query_ids = query_df['QUERY_ID'].tolist()
            selected_query_id = st.selectbox("Select a Query ID for LLM Analysis", [''] + query_ids)

            if selected_query_id:
                selected_query_data = query_df[query_df['QUERY_ID'] == selected_query_id].iloc[0]
                selected_query_text = selected_query_data['QUERY_TEXT']
                st.code(selected_query_text, language='sql')

                # Display native Snowflake insights if available for the selected query
                if pd.notna(selected_query_data.get('INSIGHT_TYPE_ID')):
                    st.info(f"**Snowflake Insight Found:**\n"
                            f"Type: `{selected_query_data['INSIGHT_TYPE_ID']}`\n"
                            f"Message: `{selected_query_data['MESSAGE']}`\n"
                            f"Suggestions: `{selected_query_data['SUGGESTIONS']}`")

                if st.button("Get LLM Suggestions for this Query"):
                    # This is a placeholder for an actual LLM API call.
                    # In a real scenario, you would send selected_query_text to an LLM API
                    # (e.g., Google Gemini API) and display its response.
                    # You would typically use st.secrets to store your LLM API key.
                    with st.spinner("Getting LLM suggestions... (This is a simulated response)"):
                        import time
                        time.sleep(3) # Simulate API call delay

                        llm_suggestion = f"""
                        **LLM Analysis for Query ID: {selected_query_id}**

                        Based on common Snowflake best practices, here are some potential optimizations for the provided query:

                        1.  **Consider using `QUALIFY` for window functions:** If the query involves ranking or deduplication using `ROW_NUMBER()`, `RANK()`, etc., consider using `QUALIFY` instead of a subquery for better readability and sometimes performance.
                        2.  **Review `JOIN` conditions:** Ensure all `JOIN` clauses are correctly specified and that appropriate columns are indexed (or clustered in Snowflake's case) to facilitate efficient joining.
                        3.  **Filter early:** Push down filters (`WHERE` clauses) as early as possible in the query execution plan to reduce the amount of data processed by subsequent operations.
                        4.  **Materialized View Candidate:** If this query is run frequently on a large, relatively static dataset, consider creating a **Materialized View** to pre-compute and store the results. This can significantly speed up subsequent executions.
                        5.  **Warehouse Size:** If this query consistently performs poorly, consider if the assigned warehouse size (`{selected_query_data.get('WAREHOUSE_SIZE', 'N/A')}`) is appropriate for the complexity and data volume.
                        6.  **Avoid `SELECT *` in subqueries:** Explicitly list only the columns needed to reduce data transfer and processing.
                        7.  **Optimize `LIKE` patterns:** If the query uses `LIKE '%value%'`, consider if a more specific pattern (`value%`) can be used, or if a full-text search solution is more appropriate for complex text matching.
                        8.  **Prefer `UNION ALL` over `UNION`:** If duplicate removal is not required, `UNION ALL` is more performant as it avoids the overhead of sorting.
                        9.  **Review UDF/Stored Procedure Efficiency:** If the query calls UDFs or stored procedures, ensure their internal logic is optimized.

                        *Note: This is a simulated LLM response. A real LLM integration would provide more dynamic and context-aware suggestions.*
                        """
                        st.markdown(llm_suggestion)
        else:
            st.info("No query history available to provide LLM insights.")

    else:
        st.info("No query history data available for insights.")

# --- General Health Check Necessities ---
st.sidebar.markdown("---")
st.sidebar.header("Health Check Best Practices")
st.sidebar.markdown("""
    For a comprehensive Snowflake health check, consider monitoring:
    * **Failed Logins:** Track unauthorized access attempts.
    * **Data Storage Usage:** Monitor overall storage consumption and growth.
    * **Pipe Usage:** For continuous data loading, monitor pipe status and errors.
    * **Task History:** Track scheduled task executions and failures.
    * **Access Control:** Regularly review roles, grants, and user permissions.
    * **Billing & Cost:** Keep an eye on credit consumption trends.
    * **Alerting:** Set up alerts for critical events (e.g., high credit usage, failed tasks, long-running queries).
""")

st.sidebar.markdown("---")
st.sidebar.info("Data refreshed every 10 minutes (cached).")
